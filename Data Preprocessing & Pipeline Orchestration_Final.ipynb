{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d764730-ed7d-4f1b-a77e-923a97c0a32f",
   "metadata": {},
   "source": [
    "Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941807d-9513-4b14-bd8d-b591b673f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy pyspark nltk airflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56a3c67-4fd0-4801-8260-feaaa4c80ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure Twitter API\n",
    "import tweepy\n",
    "import json\n",
    "\n",
    "# Replace these with your own Twitter API keys and tokens\n",
    "consumer_key = 'your_consumer_key'\n",
    "consumer_secret = 'your_consumer_secret'\n",
    "access_token = 'your_access_token'\n",
    "access_token_secret = 'your_access_token_secret'\n",
    "\n",
    "# Set up tweepy authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "# Collect tweets with a specific hashtag\n",
    "hashtag = \"#examplehashtag\"\n",
    "tweets = tweepy.Cursor(api.search_tweets, q=hashtag, lang=\"en\", tweet_mode='extended').items(1000)\n",
    "\n",
    "# Extract data\n",
    "data = []\n",
    "for tweet in tweets:\n",
    "    data.append(tweet.full_text)\n",
    "\n",
    "# Save data to a file\n",
    "with open('twitter_data.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in data:\n",
    "        f.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2058a8-152c-4ce3-b7d6-c65a1302a159",
   "metadata": {},
   "source": [
    "Data Preprocessing with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa83d78-3cd2-4600-b8e3-e42442002846",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName('TwitterSentimentAnalysis').getOrCreate()\n",
    "\n",
    "# Read data from file\n",
    "data = spark.read.text('twitter_data.txt')\n",
    "data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ccb0c01-1ed0-4aa4-a8c5-d6ce6bc6efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean and Preprocess Text Data\n",
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')  # Remove emojis\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "clean_text_udf = udf(lambda x: clean_text(x), StringType())\n",
    "data_cleaned = data.withColumn('cleaned_text', clean_text_udf(data['value']))\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol='cleaned_text', outputCol='words')\n",
    "data_tokenized = tokenizer.transform(data_cleaned)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
    "data_no_stopwords = remover.transform(data_tokenized)\n",
    "\n",
    "# Lemmatize text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "lemmatize_udf = udf(lambda x: lemmatize_words(x), ArrayType(StringType()))\n",
    "data_lemmatized = data_no_stopwords.withColumn('lemmatized_words', lemmatize_udf(data_no_stopwords['filtered_words']))\n",
    "\n",
    "# Convert to TF-IDF features\n",
    "hashingTF = HashingTF(inputCol='lemmatized_words', outputCol='raw_features', numFeatures=20)\n",
    "featurized_data = hashingTF.transform(data_lemmatized)\n",
    "\n",
    "idf = IDF(inputCol='raw_features', outputCol='features')\n",
    "idfModel = idf.fit(featurized_data)\n",
    "rescaled_data = idfModel.transform(featurized_data)\n",
    "\n",
    "rescaled_data.select('features').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a4f7c2-cb12-4703-bd52-0b21d91464fd",
   "metadata": {},
   "source": [
    "Feature Engineering for Sentiment Analysis Including MLFLOW Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49408e-c23c-4e0d-84cf-948d093560bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Pipeline for Model Training\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Load Sentiment140 dataset\n",
    "sentiment_data = spark.read.csv('path/to/sentiment140.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Preprocess Sentiment140 data (similar steps as above)\n",
    "sentiment_data = sentiment_data.withColumnRenamed('text', 'value')\n",
    "sentiment_data_cleaned = sentiment_data.withColumn('cleaned_text', clean_text_udf(sentiment_data['value']))\n",
    "sentiment_data_tokenized = tokenizer.transform(sentiment_data_cleaned)\n",
    "sentiment_data_no_stopwords = remover.transform(sentiment_data_tokenized)\n",
    "sentiment_data_lemmatized = sentiment_data_no_stopwords.withColumn('lemmatized_words', lemmatize_udf(sentiment_data_no_stopwords['filtered_words']))\n",
    "featurized_sentiment_data = hashingTF.transform(sentiment_data_lemmatized)\n",
    "rescaled_sentiment_data = idfModel.transform(featurized_sentiment_data)\n",
    "\n",
    "# Assuming the Sentiment140 data has a 'sentiment' column\n",
    "labeled_data = rescaled_sentiment_data.withColumn('label', sentiment_data['sentiment'])\n",
    "\n",
    "# Split the data\n",
    "(training_data, test_data) = labeled_data.randomSplit([0.8, 0.2])\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Build the Logistic Regression Model\n",
    "    lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline(stages=[hashingTF, idf, lr])\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"hashingTF_numFeatures\", 100)\n",
    "\n",
    "    # Train the model\n",
    "    model = pipeline.fit(training_data)\n",
    "\n",
    "    # Save the trained model for later use\n",
    "    model_path = \"sentiment_model\"\n",
    "    model.save(model_path)\n",
    "    mlflow.spark.log_model(model, \"model\")\n",
    "\n",
    "    # Load the trained model\n",
    "    model = PipelineModel.load(model_path)\n",
    "\n",
    "    # Test the model\n",
    "    predictions = model.transform(test_data)\n",
    "    predictions.select('prediction', 'label', 'features').show()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Data model for request\n",
    "class TextData(BaseModel):\n",
    "    texts: List[str]\n",
    "#\n",
    "# Define the endpoint for sentiment analysis\n",
    "@app.post(\"/analyze-sentiment\")\n",
    "async def analyze_sentiment(data: TextData):\n",
    "    texts = data.texts\n",
    "\n",
    "    # Create a DataFrame from the input texts\n",
    "    df = spark.read.csv('path/to/sentiment140.csv', header=True, inferSchema=True)\n",
    "\n",
    "    # Process the DataFrame as per preprocessing pipeline\n",
    "    df = df.withColumnRenamed('text', 'value')\n",
    "    df_cleaned = df.withColumn('cleaned_text', clean_text_udf(df['text']))\n",
    "    df_tokenized = tokenizer.transform(df_cleaned)\n",
    "    df_no_stopwords = remover.transform(df_tokenized)\n",
    "    df_lemmatized = df_no_stopwords.withColumn('lemmatized_words', lemmatize_udf(df_no_stopwords['filtered_words']))\n",
    "    df_features = hashingTF.transform(df_lemmatized)\n",
    "    df_rescaled = idfModel.transform(df_features)\n",
    "\n",
    "    # Make predictions using developed model\n",
    "    predictions = model.transform(df_rescaled)\n",
    "    results = predictions.select('prediction').collect()\n",
    "\n",
    "    # Map predictions to sentiment labels\n",
    "    sentiment_mapping = {0: 'negative', 1: 'positive'}  #Labels can be adjusted accordingly\n",
    "    sentiments = [sentiment_mapping[int(row.prediction)] for row in results]\n",
    "\n",
    "    return {\"sentiments\": sentiments}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83451cd0-b888-4281-a9e8-dae1217d4d3c",
   "metadata": {},
   "source": [
    "Pipeline Orchestration with Apache Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6a79c-d41f-4540-a6f7-395dc4aa5832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the database\n",
    "airflow db init\n",
    "\n",
    "# Create a user\n",
    "airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com\n",
    "\n",
    "# Start the Airflow web server\n",
    "airflow webserver --port 8080\n",
    "\n",
    "# Start the Airflow scheduler\n",
    "airflow scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00141ec-343c-4fe4-9852-5007932ca831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': days_ago(1),\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'twitter_sentiment_analysis',\n",
    "    default_args=default_args,\n",
    "    description='A simple Twitter sentiment analysis DAG',\n",
    "    schedule_interval='@daily',\n",
    ")\n",
    "\n",
    "def collect_data():\n",
    "    import tweepy\n",
    "    # Replace these with your own Twitter API keys and tokens\n",
    "    consumer_key = 'your_consumer_key'\n",
    "    consumer_secret = 'your_consumer_secret'\n",
    "    access_token = 'your_access_token'\n",
    "    access_token_secret = 'your_access_token_secret'\n",
    "\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "    hashtag = \"#examplehashtag\"\n",
    "    tweets = tweepy.Cursor(api.search_tweets, q=hashtag, lang=\"en\", tweet_mode='extended').items(1000)\n",
    "\n",
    "    data = []\n",
    "    for tweet in tweets:\n",
    "        data.append(tweet.full_text)\n",
    "\n",
    "    with open('/path/to/twitter_data.txt', 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "def preprocess_data():\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import StringType, ArrayType\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "    import re\n",
    "\n",
    "    spark = SparkSession.builder.appName('TwitterSentimentAnalysis').getOrCreate()\n",
    "    data = spark.read.text('/path/to/twitter_data.txt')\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "        text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    clean_text_udf = udf(lambda x: clean_text(x), StringType())\n",
    "    data_cleaned = data.withColumn('cleaned_text', clean_text_udf(data['value']))\n",
    "\n",
    "    tokenizer = Tokenizer(inputCol='cleaned_text', outputCol='words')\n",
    "    data_tokenized = tokenizer.transform(data_cleaned)\n",
    "\n",
    "    remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
    "    data_no_stopwords = remover.transform(data_tokenized)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def lemmatize_words(words):\n",
    "        return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    lemmatize_udf = udf(lambda x: lemmatize_words(x), ArrayType(StringType()))\n",
    "    data_lemmatized = data_no_stopwords.withColumn('lemmatized_words', lemmatize_udf(data_no_stopwords['filtered_words']))\n",
    "\n",
    "    hashingTF = HashingTF(inputCol='lemmatized_words', outputCol='raw_features', numFeatures=20)\n",
    "    featurized_data = hashingTF.transform(data_lemmatized)\n",
    "\n",
    "    idf = IDF(inputCol='raw_features', outputCol='features')\n",
    "    idfModel = idf.fit(featurized_data)\n",
    "    rescaled_data = idfModel.transform(featurized_data)\n",
    "\n",
    "    rescaled_data.write.save('/path/to/preprocessed_data', format='parquet')\n",
    "\n",
    "def train_model():\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    from pyspark.ml import Pipeline\n",
    "\n",
    "    spark = SparkSession.builder.appName('TwitterSentimentAnalysis').getOrCreate()\n",
    "    rescaled_data = spark.read.load('/path/to/preprocessed_data')\n",
    "\n",
    "    sentiment_data = spark.read.csv('path/to/sentiment140.csv', header=True, inferSchema=True)\n",
    "    sentiment_data = sentiment_data.withColumnRenamed('text', 'value')\n",
    "    sentiment_data_cleaned = sentiment_data.withColumn('cleaned_text', clean_text_udf(sentiment_data['value']))\n",
    "    sentiment_data_tokenized = tokenizer.transform(sentiment_data_cleaned)\n",
    "    sentiment_data_no_stopwords = remover.transform(sentiment_data_tokenized)\n",
    "    sentiment_data_lemmatized = sentiment_data_no_stopwords.withColumn('lemmatized_words', lemmatize_udf(sentiment_data_no_stopwords['filtered_words']))\n",
    "    featurized_sentiment_data = hashingTF.transform(sentiment_data_lemmatized)\n",
    "    rescaled_sentiment_data = idfModel.transform(featurized_sentiment_data)\n",
    "    labeled_data = rescaled_sentiment_data.withColumn('label', sentiment_data['sentiment'])\n",
    "\n",
    "    (training_data, test_data) = labeled_data.randomSplit([0.8, 0.2])\n",
    "\n",
    "    lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "    pipeline = Pipeline(stages=[hashingTF, idf, lr])\n",
    "\n",
    "    model = pipeline.fit(training_data)\n",
    "    predictions = model.transform(test_data)\n",
    "    predictions.select('prediction', 'label', 'features').show()\n",
    "\n",
    "collect_data_task = PythonOperator(\n",
    "    task_id='collect_data',\n",
    "    python_callable=collect_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "preprocess_data_task = PythonOperator(\n",
    "    task_id='preprocess_data',\n",
    "    python_callable=preprocess_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "train_model_task = PythonOperator(\n",
    "    task_id='train_model',\n",
    "    python_callable=train_model,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "collect_data_task >> preprocess_data_task >> train_model_task\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
