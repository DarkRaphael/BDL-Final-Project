{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d764730-ed7d-4f1b-a77e-923a97c0a32f",
   "metadata": {},
   "source": [
    "Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4941807d-9513-4b14-bd8d-b591b673f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy pyspark nltk airflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f56a3c67-4fd0-4801-8260-feaaa4c80ccf",
   "metadata": {},
   "outputs": [
    {
     "ename": "Unauthorized",
     "evalue": "401 Unauthorized\n89 - Invalid or expired token.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnauthorized\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#print(tweets)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Extract data\u001b[39;00m\n\u001b[1;32m     23\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets:\n\u001b[1;32m     25\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(tweet\u001b[38;5;241m.\u001b[39mfull_text)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Save data to a file\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tweepy/cursor.py:86\u001b[0m, in \u001b[0;36mBaseIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tweepy/cursor.py:286\u001b[0m, in \u001b[0;36mItemIterator.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_page \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_page) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Reached end of current page, get the next page...\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_page) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_page \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpage_iterator)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tweepy/cursor.py:86\u001b[0m, in \u001b[0;36mBaseIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tweepy/cursor.py:167\u001b[0m, in \u001b[0;36mIdIterator.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRawParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     model \u001b[38;5;241m=\u001b[39m ModelParser()\u001b[38;5;241m.\u001b[39mparse(\n\u001b[1;32m    170\u001b[0m         data, api \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m,\n\u001b[1;32m    171\u001b[0m         payload_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mpayload_list,\n\u001b[1;32m    172\u001b[0m         payload_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mpayload_type\n\u001b[1;32m    173\u001b[0m     )\n\u001b[1;32m    174\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mparse(\n\u001b[1;32m    175\u001b[0m         data, api \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m,\n\u001b[1;32m    176\u001b[0m         payload_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mpayload_list,\n\u001b[1;32m    177\u001b[0m         payload_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39mpayload_type\n\u001b[1;32m    178\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tweepy/api.py:33\u001b[0m, in \u001b[0;36mpagination.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(method)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tweepy/api.py:46\u001b[0m, in \u001b[0;36mpayload.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayload_list\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m payload_list\n\u001b[1;32m     45\u001b[0m kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpayload_type\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m payload_type\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tweepy/api.py:1146\u001b[0m, in \u001b[0;36mAPI.search_tweets\u001b[0;34m(self, q, **kwargs)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;129m@pagination\u001b[39m(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;129m@payload\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msearch_results\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch_tweets\u001b[39m(\u001b[38;5;28mself\u001b[39m, q, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"search_tweets(q, *, geocode, lang, locale, result_type, count, \\\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;124;03m                     until, since_id, max_id, include_entities)\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;124;03m    .. _Twitter's documentation on the standard search API: https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/overview\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearch/tweets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mq\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgeocode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlang\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresult_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muntil\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msince_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minclude_entities\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tweepy/api.py:269\u001b[0m, in \u001b[0;36mAPI.request\u001b[0;34m(self, method, endpoint, endpoint_parameters, params, headers, json_payload, parser, payload_list, payload_type, post_data, files, require_auth, return_cursors, upload_api, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequest(resp)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Unauthorized(resp)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Forbidden(resp)\n",
      "\u001b[0;31mUnauthorized\u001b[0m: 401 Unauthorized\n89 - Invalid or expired token."
     ]
    }
   ],
   "source": [
    "#Configure Twitter API\n",
    "import tweepy\n",
    "import json\n",
    "\n",
    "# Replace these with your own Twitter API keys and tokens\n",
    "consumer_key = 'your_consumer_key'\n",
    "consumer_secret = 'your_consumer_secret'\n",
    "access_token = 'your_access_token'\n",
    "access_token_secret = 'your_access_token_secret'\n",
    "\n",
    "# Set up tweepy authentication\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "# Collect tweets with a specific hashtag\n",
    "hashtag = \"#competition\"\n",
    "tweets = tweepy.Cursor(api.search_tweets, q=hashtag, lang=\"en\", tweet_mode='extended').items(1000)\n",
    "\n",
    "print(tweets)\n",
    "\n",
    "# Extract data\n",
    "data = []\n",
    "for tweet in tweets:\n",
    "    data.append(tweet.full_text)\n",
    "\n",
    "# Save data to a file\n",
    "with open('twitter_data.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in data:\n",
    "        f.write(\"%s\\n\" % item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2058a8-152c-4ce3-b7d6-c65a1302a159",
   "metadata": {},
   "source": [
    "Data Preprocessing with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfa83d78-3cd2-4600-b8e3-e42442002846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JAVA_HOME is not set\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Initialize Spark session\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTwitterSentimentAnalysis\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Read data from file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mtext(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwitter_data.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[1;32m    500\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 515\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    199\u001b[0m     )\n\u001b[0;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    204\u001b[0m         master,\n\u001b[1;32m    205\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    216\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[1;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName('TwitterSentimentAnalysis').getOrCreate()\n",
    "\n",
    "# Read data from file\n",
    "data = spark.read.text('twitter_data.txt')\n",
    "data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ccb0c01-1ed0-4aa4-a8c5-d6ce6bc6efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean and Preprocess Text Data\n",
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')  # Remove emojis\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "clean_text_udf = udf(lambda x: clean_text(x), StringType())\n",
    "data_cleaned = data.withColumn('cleaned_text', clean_text_udf(data['value']))\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol='cleaned_text', outputCol='words')\n",
    "data_tokenized = tokenizer.transform(data_cleaned)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
    "data_no_stopwords = remover.transform(data_tokenized)\n",
    "\n",
    "# Lemmatize text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "lemmatize_udf = udf(lambda x: lemmatize_words(x), ArrayType(StringType()))\n",
    "data_lemmatized = data_no_stopwords.withColumn('lemmatized_words', lemmatize_udf(data_no_stopwords['filtered_words']))\n",
    "\n",
    "# Convert to TF-IDF features\n",
    "hashingTF = HashingTF(inputCol='lemmatized_words', outputCol='raw_features', numFeatures=20)\n",
    "featurized_data = hashingTF.transform(data_lemmatized)\n",
    "\n",
    "idf = IDF(inputCol='raw_features', outputCol='features')\n",
    "idfModel = idf.fit(featurized_data)\n",
    "rescaled_data = idfModel.transform(featurized_data)\n",
    "\n",
    "rescaled_data.select('features').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09efcb1b-fc1f-4fba-a631-6d05a0d20d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/saketh/nltk_data...\n",
      "[Stage 38:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(20,[1,5,10,11,12...|\n",
      "|(20,[1,4,6,7,10,1...|\n",
      "|(20,[0,1,5,7,8,11...|\n",
      "|(20,[1,7,10,11,15...|\n",
      "|(20,[4,6,12,14,15...|\n",
      "|(20,[7,18],[1.241...|\n",
      "|(20,[17,18],[1.06...|\n",
      "|(20,[4,7,8,9,11,1...|\n",
      "|(20,[14],[1.36014...|\n",
      "|(20,[7],[3.724169...|\n",
      "|(20,[4,7,8,11],[1...|\n",
      "|(20,[4,6],[1.2330...|\n",
      "|(20,[5,8,9,11,13,...|\n",
      "|(20,[1,5,7,12,18,...|\n",
      "|(20,[4,6,10,12,17...|\n",
      "|(20,[2,8,10,11,14...|\n",
      "|(20,[2,3,4,6,8,9,...|\n",
      "|(20,[7,8],[1.2413...|\n",
      "|(20,[0,3,9,11,12,...|\n",
      "|(20,[2,4,8,9,11,1...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndata_cleaned = data.withColumn('cleaned_text', clean_text_udf(data['value']))\\n\\n# Tokenize text\\ntokenizer = Tokenizer(inputCol='cleaned_text', outputCol='words')\\ndata_tokenized = tokenizer.transform(data_cleaned)\\n\\n# Remove stop words\\nremover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\\ndata_no_stopwords = remover.transform(data_tokenized)\\n\\n# Lemmatize text\\nlemmatizer = WordNetLemmatizer()\\n\\ndef lemmatize_words(words):\\n    return [lemmatizer.lemmatize(word) for word in words]\\n\\nlemmatize_udf = udf(lambda x: lemmatize_words(x), ArrayType(StringType()))\\ndata_lemmatized = data_no_stopwords.withColumn('lemmatized_words', lemmatize_udf(data_no_stopwords['filtered_words']))\\n\\n# Convert to TF-IDF features\\nhashingTF = HashingTF(inputCol='lemmatized_words', outputCol='raw_features', numFeatures=20)\\nfeaturized_data = hashingTF.transform(data_lemmatized)\\n\\nidf = IDF(inputCol='raw_features', outputCol='features')\\nidfModel = idf.fit(featurized_data)\\nrescaled_data = idfModel.transform(featurized_data)\\n\\nrescaled_data.select('features').show()\\n\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean and Preprocess Text Data\n",
    "import re\n",
    "import nltk\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName('TwitterSentimentAnalysis').getOrCreate()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"label\", IntegerType()),\n",
    "    StructField(\"ids\", IntegerType()),\n",
    "    StructField(\"date\", StringType()),\n",
    "    StructField(\"flag\", StringType()),\n",
    "    StructField(\"user\", StringType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "])\n",
    "\n",
    "# Load Sentiment140 dataset\n",
    "sentiment_data = spark.read.csv('sentiment140.csv', header=False, schema=schema)\n",
    "\n",
    "#column_length = sentiment_data.select('label').count()\n",
    "#len04 = sentiment_data.select(\"label\").filter(sentiment_data['label'] == 4).count()\n",
    "#print(column_length, len04)\n",
    "# first 800000 points are labelled with '0' and the remaining 800000 are labelled '4', instances of class '2' are not present in the dataset\n",
    "\n",
    "# Clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')  # Remove emojis\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "clean_text_udf = udf(lambda x: clean_text(x), StringType())\n",
    "sentiment_data = sentiment_data.withColumnRenamed('text', 'value')\n",
    "sentiment_data_cleaned = sentiment_data.withColumn('cleaned_text', clean_text_udf(sentiment_data['value']))\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol='cleaned_text', outputCol='words')\n",
    "sentiment_data_tokenized = tokenizer.transform(sentiment_data_cleaned)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
    "sentiment_data_no_stopwords = remover.transform(sentiment_data_tokenized)\n",
    "\n",
    "# Lemmatize text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatize_udf = udf(lambda x: lemmatize_words(x), ArrayType(StringType()))\n",
    "sentiment_data_lemmatized = sentiment_data_no_stopwords.withColumn('lemmatized_words', lemmatize_udf(sentiment_data_no_stopwords['filtered_words']))\n",
    "\n",
    "# Convert to TF-IDF features\n",
    "hashingTF = HashingTF(inputCol='lemmatized_words', outputCol='raw_features', numFeatures=20)\n",
    "featurized_sentiment_data = hashingTF.transform(sentiment_data_lemmatized)\n",
    "\n",
    "idf = IDF(inputCol='raw_features', outputCol='features')\n",
    "idfModel = idf.fit(featurized_sentiment_data)\n",
    "rescaled_sentiment_data = idfModel.transform(featurized_sentiment_data)\n",
    "\n",
    "rescaled_sentiment_data.select('features').show()\n",
    "\n",
    "\n",
    "#featurized_sentiment_data = hashingTF.transform(sentiment_data_lemmatized)\n",
    "#rescaled_sentiment_data = idfModel.transform(featurized_sentiment_data)\n",
    "\n",
    "'''\n",
    "data_cleaned = data.withColumn('cleaned_text', clean_text_udf(data['value']))\n",
    "\n",
    "# Tokenize text\n",
    "tokenizer = Tokenizer(inputCol='cleaned_text', outputCol='words')\n",
    "data_tokenized = tokenizer.transform(data_cleaned)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
    "data_no_stopwords = remover.transform(data_tokenized)\n",
    "\n",
    "# Lemmatize text\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "lemmatize_udf = udf(lambda x: lemmatize_words(x), ArrayType(StringType()))\n",
    "data_lemmatized = data_no_stopwords.withColumn('lemmatized_words', lemmatize_udf(data_no_stopwords['filtered_words']))\n",
    "\n",
    "# Convert to TF-IDF features\n",
    "hashingTF = HashingTF(inputCol='lemmatized_words', outputCol='raw_features', numFeatures=20)\n",
    "featurized_data = hashingTF.transform(data_lemmatized)\n",
    "\n",
    "idf = IDF(inputCol='raw_features', outputCol='features')\n",
    "idfModel = idf.fit(featurized_data)\n",
    "rescaled_data = idfModel.transform(featurized_data)\n",
    "\n",
    "rescaled_data.select('features').show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19d3b204-d9c4-47c0-a2fc-64a537356c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Specify the path where you want to save the IDF model\n",
    "idf_model_path = \"idf_model\"\n",
    "\n",
    "# Save the IDF model\n",
    "idfModel.save(idf_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf621add-4230-41db-9164-99487a7782c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNSUPPORTED_DATA_TYPE_FOR_DATASOURCE] The CSV datasource doesn't support the column `features` of the type \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrescaled_sentiment_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrescaled_sentiment_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfeatures\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1863\u001b[0m )\n\u001b[0;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNSUPPORTED_DATA_TYPE_FOR_DATASOURCE] The CSV datasource doesn't support the column `features` of the type \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\"."
     ]
    }
   ],
   "source": [
    "output_path = \"rescaled_sentiment_data.csv\"\n",
    "rescaled_sentiment_data.select('features').write.option(\"header\", \"true\").csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a4f7c2-cb12-4703-bd52-0b21d91464fd",
   "metadata": {},
   "source": [
    "Feature Engineering for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f49408e-c23c-4e0d-84cf-948d093560bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#Create Pipeline for Model Training\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Load Sentiment140 dataset\n",
    "sentiment_data = spark.read.csv('path/to/sentiment140.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Preprocess Sentiment140 data (similar steps as above)\n",
    "sentiment_data = sentiment_data.withColumnRenamed('text', 'value')\n",
    "sentiment_data_cleaned = sentiment_data.withColumn('cleaned_text', clean_text_udf(sentiment_data['value']))\n",
    "sentiment_data_tokenized = tokenizer.transform(sentiment_data_cleaned)\n",
    "sentiment_data_no_stopwords = remover.transform(sentiment_data_tokenized)\n",
    "sentiment_data_lemmatized = sentiment_data_no_stopwords.withColumn('lemmatized_words', lemmatize_udf(sentiment_data_no_stopwords['filtered_words']))\n",
    "featurized_sentiment_data = hashingTF.transform(sentiment_data_lemmatized)\n",
    "rescaled_sentiment_data = idfModel.transform(featurized_sentiment_data)\n",
    "\n",
    "# Assuming the Sentiment140 data has a 'sentiment' column\n",
    "labeled_data = rescaled_sentiment_data.withColumn('label', sentiment_data['sentiment'])\n",
    "\n",
    "# Split the data\n",
    "(training_data, test_data) = labeled_data.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Build the Logistic Regression Model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label', family='multinomial')\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline(stages=[hashingTF, idf, lr])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(training_data)\n",
    "\n",
    "# Test the model\n",
    "predictions = model.transform(test_data)\n",
    "predictions.select('prediction', 'label', 'features').show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239d3ee5-eb89-447d-9804-1ea914f92a61",
   "metadata": {},
   "source": [
    "ML FLOW TRACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ec233-66a8-4f1c-ab44-e83305c5e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Pipeline for Model Training\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "'''\n",
    "# Load Sentiment140 dataset\n",
    "sentiment_data = spark.read.csv('path/to/sentiment140.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Preprocess Sentiment140 data (similar steps as above)\n",
    "sentiment_data = sentiment_data.withColumnRenamed('text', 'value')\n",
    "sentiment_data_cleaned = sentiment_data.withColumn('cleaned_text', clean_text_udf(sentiment_data['value']))\n",
    "sentiment_data_tokenized = tokenizer.transform(sentiment_data_cleaned)\n",
    "sentiment_data_no_stopwords = remover.transform(sentiment_data_tokenized)\n",
    "sentiment_data_lemmatized = sentiment_data_no_stopwords.withColumn('lemmatized_words', lemmatize_udf(sentiment_data_no_stopwords['filtered_words']))\n",
    "featurized_sentiment_data = hashingTF.transform(sentiment_data_lemmatized)\n",
    "rescaled_sentiment_data = idfModel.transform(featurized_sentiment_data)\n",
    "\n",
    "# Assuming the Sentiment140 data has a 'sentiment' column\n",
    "labeled_data = rescaled_sentiment_data.withColumn('label', sentiment_data['target'])\n",
    "'''\n",
    "\n",
    "# Split the data\n",
    "(training_data, test_data) = labeled_data.randomSplit([0.8, 0.2])\n",
    "\n",
    "# MLflow experiment tracking\n",
    "mlflow.set_experiment(\"Sentiment Analysis\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    \n",
    "    # Build the Logistic Regression Model\n",
    "    lr = LogisticRegression(featuresCol='features', labelCol='label', family='multinomial')\n",
    "    \n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline(stages=[hashingTF, idf, lr])\n",
    "    \n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"hashingTF_numFeatures\", 100)\n",
    "    \n",
    "    # Train the model\n",
    "    model = pipeline.fit(training_data)\n",
    "    \n",
    "    # Save the trained model for later use\n",
    "    model_path = \"sentiment_model\"\n",
    "    model.save(model_path)\n",
    "    mlflow.spark.log_model(model, \"model\")\n",
    "    \n",
    "    # Load the trained model\n",
    "    model = PipelineModel.load(model_path)\n",
    "    \n",
    "    # Test the model\n",
    "    predictions = model.transform(test_data)\n",
    "    predictions.select('prediction', 'label', 'features').show()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855df657-0e7e-4a56-be41-06feaad6195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Data model for request\n",
    "class TextData(BaseModel):\n",
    "    texts: List[str]\n",
    "\n",
    "# Define the endpoint for sentiment analysis\n",
    "@app.post(\"/analyze-sentiment\")\n",
    "async def analyze_sentiment(data: TextData):\n",
    "    texts = data.texts\n",
    "\n",
    "    # Create a DataFrame from the input texts\n",
    "    df = spark.createDataFrame([(text,) for text in texts], [\"text\"])\n",
    "\n",
    "    # Process the DataFrame as per preprocessing pipeline\n",
    "    df_cleaned = df.withColumn('cleaned_text', clean_text_udf(df['text']))\n",
    "    df_tokenized = tokenizer.transform(df_cleaned)\n",
    "    df_no_stopwords = remover.transform(df_tokenized)\n",
    "    df_lemmatized = df_no_stopwords.withColumn('lemmatized_words', lemmatize_udf(df_no_stopwords['filtered_words']))\n",
    "    df_features = hashingTF.transform(df_lemmatized)\n",
    "    df_rescaled = idfModel.transform(df_features)\n",
    "\n",
    "    # Make predictions using developed model\n",
    "    predictions = model.transform(df_rescaled)\n",
    "    results = predictions.select('prediction').collect()\n",
    "\n",
    "    # Map predictions to sentiment labels\n",
    "    sentiment_mapping = {0: 'negative', 1: 'positive'}  #Labels can be adjusted accordingly\n",
    "    sentiments = [sentiment_mapping[int(row.prediction)] for row in results]\n",
    "\n",
    "    return {\"sentiments\": sentiments}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83451cd0-b888-4281-a9e8-dae1217d4d3c",
   "metadata": {},
   "source": [
    "Pipeline Orchestration with Apache Airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6a79c-d41f-4540-a6f7-395dc4aa5832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the database\n",
    "airflow db init\n",
    "\n",
    "# Create a user\n",
    "airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com\n",
    "\n",
    "# Start the Airflow web server\n",
    "airflow webserver --port 8080\n",
    "\n",
    "# Start the Airflow scheduler\n",
    "airflow scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00141ec-343c-4fe4-9852-5007932ca831",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': days_ago(1),\n",
    "    'retries': 1,\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'twitter_sentiment_analysis',\n",
    "    default_args=default_args,\n",
    "    description='A simple Twitter sentiment analysis DAG',\n",
    "    schedule_interval='@daily',\n",
    ")\n",
    "\n",
    "def collect_data():\n",
    "    import tweepy\n",
    "    # Replace these with your own Twitter API keys and tokens\n",
    "    consumer_key = 'your_consumer_key'\n",
    "    consumer_secret = 'your_consumer_secret'\n",
    "    access_token = 'your_access_token'\n",
    "    access_token_secret = 'your_access_token_secret'\n",
    "\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "    hashtag = \"#examplehashtag\"\n",
    "    tweets = tweepy.Cursor(api.search_tweets, q=hashtag, lang=\"en\", tweet_mode='extended').items(1000)\n",
    "\n",
    "    data = []\n",
    "    for tweet in tweets:\n",
    "        data.append(tweet.full_text)\n",
    "\n",
    "    with open('/path/to/twitter_data.txt', 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "\n",
    "def preprocess_data():\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import StringType, ArrayType\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
    "    import re\n",
    "\n",
    "    spark = SparkSession.builder.appName('TwitterSentimentAnalysis').getOrCreate()\n",
    "    data = spark.read.text('/path/to/twitter_data.txt')\n",
    "\n",
    "    def clean_text(text):\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\@\\w+|\\#', '', text)\n",
    "        text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    clean_text_udf = udf(lambda x: clean_text(x), StringType())\n",
    "    data_cleaned = data.withColumn('cleaned_text', clean_text_udf(data['value']))\n",
    "\n",
    "    tokenizer = Tokenizer(inputCol='cleaned_text', outputCol='words')\n",
    "    data_tokenized = tokenizer.transform(data_cleaned)\n",
    "\n",
    "    remover = StopWordsRemover(inputCol='words', outputCol='filtered_words')\n",
    "    data_no_stopwords = remover.transform(data_tokenized)\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def lemmatize_words(words):\n",
    "        return [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    lemmatize_udf = udf(lambda x: lemmatize_words(x), ArrayType(StringType()))\n",
    "    data_lemmatized = data_no_stopwords.withColumn('lemmatized_words', lemmatize_udf(data_no_stopwords['filtered_words']))\n",
    "\n",
    "    hashingTF = HashingTF(inputCol='lemmatized_words', outputCol='raw_features', numFeatures=20)\n",
    "    featurized_data = hashingTF.transform(data_lemmatized)\n",
    "\n",
    "    idf = IDF(inputCol='raw_features', outputCol='features')\n",
    "    idfModel = idf.fit(featurized_data)\n",
    "    rescaled_data = idfModel.transform(featurized_data)\n",
    "\n",
    "    rescaled_data.write.save('/path/to/preprocessed_data', format='parquet')\n",
    "\n",
    "def train_model():\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.ml.classification import LogisticRegression\n",
    "    from pyspark.ml import Pipeline\n",
    "\n",
    "    spark = SparkSession.builder.appName('TwitterSentimentAnalysis').getOrCreate()\n",
    "    rescaled_data = spark.read.load('/path/to/preprocessed_data')\n",
    "\n",
    "    sentiment_data = spark.read.csv('path/to/sentiment140.csv', header=True, inferSchema=True)\n",
    "    sentiment_data = sentiment_data.withColumnRenamed('text', 'value')\n",
    "    sentiment_data_cleaned = sentiment_data.withColumn('cleaned_text', clean_text_udf(sentiment_data['value']))\n",
    "    sentiment_data_tokenized = tokenizer.transform(sentiment_data_cleaned)\n",
    "    sentiment_data_no_stopwords = remover.transform(sentiment_data_tokenized)\n",
    "    sentiment_data_lemmatized = sentiment_data_no_stopwords.withColumn('lemmatized_words', lemmatize_udf(sentiment_data_no_stopwords['filtered_words']))\n",
    "    featurized_sentiment_data = hashingTF.transform(sentiment_data_lemmatized)\n",
    "    rescaled_sentiment_data = idfModel.transform(featurized_sentiment_data)\n",
    "    labeled_data = rescaled_sentiment_data.withColumn('label', sentiment_data['sentiment'])\n",
    "\n",
    "    (training_data, test_data) = labeled_data.randomSplit([0.8, 0.2])\n",
    "\n",
    "    lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "    pipeline = Pipeline(stages=[hashingTF, idf, lr])\n",
    "\n",
    "    model = pipeline.fit(training_data)\n",
    "    predictions = model.transform(test_data)\n",
    "    predictions.select('prediction', 'label', 'features').show()\n",
    "\n",
    "collect_data_task = PythonOperator(\n",
    "    task_id='collect_data',\n",
    "    python_callable=collect_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "preprocess_data_task = PythonOperator(\n",
    "    task_id='preprocess_data',\n",
    "    python_callable=preprocess_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "train_model_task = PythonOperator(\n",
    "    task_id='train_model',\n",
    "    python_callable=train_model,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "collect_data_task >> preprocess_data_task >> train_model_task\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
